{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31e1e64a-9cf0-4de2-b956-50ddc576db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e6df831-1c49-4807-8e1b-a0e55b0b0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgclient = bigquery.Client ()\n",
    "gsclient = storage.Client ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f12c330-435b-40a0-a97a-b5a1be02de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"udemydataengineering\"\n",
    "dataset = \"udemy_retail_gcpdataset\"\n",
    "table1 = f\"{project}.{dataset}.udemy_retail_gcptableorderitem\"\n",
    "table2 = f\"{project}.{dataset}.udemy_retail_gcptableproduct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01498e4f-7a03-4c28-b111-45ee8738741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help (bgclient)\n",
    "#help (gsclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2aa2460e-e943-4e4d-ac13-f5f26801fc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bucket = \"udemy-retail-gcpbucket\"\n",
    "folder = \"udemy-pythondemo/retail-db/retail_db/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d751823c-a209-47cb-9dc9-f372f962e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = list (gsclient.list_blobs (bucket))#list (gsclient.get_bucket (bucket).list_blobs ())#\n",
    "\n",
    "blobschema = {}\n",
    "\n",
    "uri1 =\"\"#f\"gs://{bucket}/{folder}/order_items/part-00000\"\n",
    "uri2 =\"\"#f\"gs://{bucket}/{folder}/products/part-00000\"\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len (blobs):\n",
    "    '''\n",
    "    print (type (blobs [i]))\n",
    "    print (blobs [i])\n",
    "    print (blobs [i].bucket.name)\n",
    "    print (blobs [i].name)\n",
    "    #print (json.load (open (f\"gs://{bucket}/{blobs [i].name}\")))\n",
    "    '''\n",
    "    if \".json\" in blobs [i].name:\n",
    "        #print (\"JSON File Found\")\n",
    "        #print  (json.loads (blobs [i].download_as_string (client=gsclient)))#have to use json.loads not json.load because have to download as string when reading from gcp storage\n",
    "        blobschema = json.loads (blobs [i].download_as_string (client=gsclient))\n",
    "    elif \"order_items\" in blobs [i].name:\n",
    "        uri1 = f\"gs://{blobs [i].bucket.name}/{blobs [i].name}\"\n",
    "    elif \"products\" in blobs [i].name:\n",
    "        uri2 = f\"gs://{blobs [i].bucket.name}/{blobs [i].name}\"\n",
    "    \n",
    "    i = i+1\n",
    "\n",
    "#help (gsclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61bb96f3-5ffd-4553-8c93-c17151cb61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (blobschema)\n",
    "schema1raw = sorted (blobschema [\"order_items\"], key=lambda data: data [\"column_position\"])\n",
    "#print (schema1raw)\n",
    "schema2raw = sorted (blobschema [\"products\"], key=lambda data: data [\"column_position\"])\n",
    "#print (schema2raw)\n",
    "\n",
    "schema1cols = [schema1coldict [\"column_name\"] for schema1coldict in schema1raw]\n",
    "#print (schema1cols)\n",
    "schema2cols = [schema2coldict [\"column_name\"] for schema2coldict in schema2raw]\n",
    "#print (schema2cols)\n",
    "\n",
    "schema1dictionary = {\"orderitem_id\": \"INTEGER\", \"orderitem_orderid\": \"INTEGER\", \"orderitem_productid\": \"INTEGER\", \"orderitem_quantity\": \"INTEGER\", \"orderitem_subtotal\": \"DECIMAL\", \"orderitem_productprice\": \"DECIMAL\"}\n",
    "schema2dictionary = {\"product_id\": \"INTEGER\", \"product_categoryid\": \"INTEGER\", \"product_name\": \"STRING\", \"product_description\": \"STRING\", \"product_price\": \"DECIMAL\", \"product_image\": \"STRING\"}\n",
    "\n",
    "schema1 = []\n",
    "schema2 = []\n",
    "\n",
    "schema1keys = list (schema1dictionary.keys ())\n",
    "i = 0\n",
    "while i < len (schema1keys):\n",
    "    schema1append = bigquery.SchemaField (schema1keys [i], schema1dictionary [schema1keys [i]])\n",
    "    schema1.append (schema1append)\n",
    "    i = i+1\n",
    "\n",
    "schema2keys = list (schema2dictionary.keys ())\n",
    "i = 0\n",
    "while i < len (schema2keys):\n",
    "    schema2append = bigquery.SchemaField (schema2keys [i], schema2dictionary [schema2keys [i]])\n",
    "    schema2.append (schema2append)\n",
    "    i = i + 1\n",
    "\n",
    "#print (schema1)\n",
    "#print (schema2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1ea9bfd2-3683-4b3b-811d-7697dbf95605",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobconfig1 = bigquery.LoadJobConfig (schema=schema1)\n",
    "jobconfig2 = bigquery.LoadJobConfig (schema=schema2)\n",
    "\n",
    "#job1 = bgclient.create_job ()\n",
    "#job2 = bgclient.create_job ()\n",
    "\n",
    "load1init = bgclient.load_table_from_uri (uri1, table1, job_config=jobconfig1)\n",
    "load2init = bgclient.load_table_from_uri (uri2, table2, job_config=jobconfig2)\n",
    "\n",
    "load1result = load1init.result ()\n",
    "load2result = load2init.result ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6e7208b5-dea5-46d8-86c1-cf6a59cfe93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.bigquery.table.RowIterator object at 0x00000266A096A440>\n",
      "<google.cloud.bigquery.table.RowIterator object at 0x00000266A11A4100>\n",
      "(168758, 67504, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(168865, 67543, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(168888, 67552, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(169154, 67658, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(169218, 67682, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(169252, 67695, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(169290, 67713, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(169355, 67740, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(169406, 67757, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(169606, 67839, 768, 1, Decimal('299.99'), Decimal('299.99'))\n",
      "(934, 42, 'Callaway X Hot Driver', None, Decimal('0'), 'http://images.acmesports.sports/Callaway+X+Hot+Driver')\n",
      "(41, 3, \"adidas Men's Mexico Home Soccer Jersey\", None, Decimal('90'), 'http://images.acmesports.sports/adidas+Men%27s+Mexico+Home+Soccer+Jersey')\n",
      "(519, 24, \"adidas Men's Mexico Home Soccer Jersey\", None, Decimal('90'), 'http://images.acmesports.sports/adidas+Men%27s+Mexico+Home+Soccer+Jersey')\n",
      "(720, 33, \"LIJA Women's Eyelet Golf Skort\", None, Decimal('90'), 'http://images.acmesports.sports/LIJA+Women%27s+Eyelet+Golf+Skort')\n",
      "(974, 44, \"The North Face Women's Stinson Rain Jacket\", None, Decimal('90'), 'http://images.acmesports.sports/The+North+Face+Women%27s+Stinson+Rain+Jacket')\n",
      "(1258, 56, \"adidas Men's Mexico Home Soccer Jersey\", None, Decimal('90'), 'http://images.acmesports.sports/adidas+Men%27s+Mexico+Home+Soccer+Jersey')\n",
      "(1264, 56, \"adidas Men's Argentina Home Soccer Jersey\", None, Decimal('90'), 'http://images.acmesports.sports/adidas+Men%27s+Argentina+Home+Soccer+Jersey')\n",
      "(1267, 56, \"adidas Men's Mexico Red Away Soccer Jersey\", None, Decimal('90'), 'http://images.acmesports.sports/adidas+Men%27s+Mexico+Red+Away+Soccer+Jersey')\n",
      "(1269, 56, \"Nike Men's Netherlands Orange Home Stadium So\", None, Decimal('90'), 'http://images.acmesports.sports/Nike+Men%27s+Netherlands+Orange+Home+Stadium+Soccer+Jersey')\n",
      "(1285, 57, \"adidas Men's Mexico Home Soccer Jersey\", None, Decimal('90'), 'http://images.acmesports.sports/adidas+Men%27s+Mexico+Home+Soccer+Jersey')\n"
     ]
    }
   ],
   "source": [
    "#help (bgclient)\n",
    "\n",
    "query1string = f\"select * from {table1} limit 10\"\n",
    "query2string = f\"select * from {table2} limit 10\"\n",
    "\n",
    "query1init = bgclient.query (query1string)\n",
    "query2init = bgclient.query (query2string)\n",
    "\n",
    "query1result = query1init.result ()\n",
    "query2result = query2init.result ()\n",
    "\n",
    "print (query1result)\n",
    "print (query2result)\n",
    "\n",
    "query1list = list (query1result)\n",
    "\n",
    "i = 0\n",
    "while i < len (query1list):\n",
    "    print (query1list [i].values ())\n",
    "    i = i+1\n",
    "\n",
    "query2list = list (query2result)\n",
    "\n",
    "i=0\n",
    "while i < len (query2list):\n",
    "    print (query2list [i].values ())\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4cc58c65-ae6a-448e-8ebc-68f95130b8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Client in module google.cloud.bigquery.client object:\n",
      "\n",
      "class Client(google.cloud.client.ClientWithProject)\n",
      " |  Client(project=None, credentials=None, _http=None, location=None, default_query_job_config=None, default_load_job_config=None, client_info=None, client_options=None) -> None\n",
      " |  \n",
      " |  Client to bundle configuration needed for API requests.\n",
      " |  \n",
      " |  Args:\n",
      " |      project (Optional[str]):\n",
      " |          Project ID for the project which the client acts on behalf of.\n",
      " |          Will be passed when creating a dataset / job. If not passed,\n",
      " |          falls back to the default inferred from the environment.\n",
      " |      credentials (Optional[google.auth.credentials.Credentials]):\n",
      " |          The OAuth2 Credentials to use for this client. If not passed\n",
      " |          (and if no ``_http`` object is passed), falls back to the\n",
      " |          default inferred from the environment.\n",
      " |      _http (Optional[requests.Session]):\n",
      " |          HTTP object to make requests. Can be any object that\n",
      " |          defines ``request()`` with the same interface as\n",
      " |          :meth:`requests.Session.request`. If not passed, an ``_http``\n",
      " |          object is created that is bound to the ``credentials`` for the\n",
      " |          current object.\n",
      " |          This parameter should be considered private, and could change in\n",
      " |          the future.\n",
      " |      location (Optional[str]):\n",
      " |          Default location for jobs / datasets / tables.\n",
      " |      default_query_job_config (Optional[google.cloud.bigquery.job.QueryJobConfig]):\n",
      " |          Default ``QueryJobConfig``.\n",
      " |          Will be merged into job configs passed into the ``query`` method.\n",
      " |      default_load_job_config (Optional[google.cloud.bigquery.job.LoadJobConfig]):\n",
      " |          Default ``LoadJobConfig``.\n",
      " |          Will be merged into job configs passed into the ``load_table_*`` methods.\n",
      " |      client_info (Optional[google.api_core.client_info.ClientInfo]):\n",
      " |          The client info used to send a user-agent string along with API\n",
      " |          requests. If ``None``, then default info will be used. Generally,\n",
      " |          you only need to set this if you're developing your own library\n",
      " |          or partner tool.\n",
      " |      client_options (Optional[Union[google.api_core.client_options.ClientOptions, Dict]]):\n",
      " |          Client options used to set user options on the client. API Endpoint\n",
      " |          should be set through client_options.\n",
      " |  \n",
      " |  Raises:\n",
      " |      google.auth.exceptions.DefaultCredentialsError:\n",
      " |          Raised if ``credentials`` is not specified and the library fails\n",
      " |          to acquire default credentials.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Client\n",
      " |      google.cloud.client.ClientWithProject\n",
      " |      google.cloud.client.Client\n",
      " |      google.cloud.client._ClientFactoryMixin\n",
      " |      google.cloud.client._ClientProjectMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback)\n",
      " |  \n",
      " |  __init__(self, project=None, credentials=None, _http=None, location=None, default_query_job_config=None, default_load_job_config=None, client_info=None, client_options=None) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  cancel_job(self, job_id: str, project: Optional[str] = None, location: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> Union[google.cloud.bigquery.job.load.LoadJob, google.cloud.bigquery.job.copy_.CopyJob, google.cloud.bigquery.job.extract.ExtractJob, google.cloud.bigquery.job.query.QueryJob]\n",
      " |      Attempt to cancel a job from a job ID.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/cancel\n",
      " |      \n",
      " |      Args:\n",
      " |          job_id (Union[                 str,                 google.cloud.bigquery.job.LoadJob,                 google.cloud.bigquery.job.CopyJob,                 google.cloud.bigquery.job.ExtractJob,                 google.cloud.bigquery.job.QueryJob             ]): Job identifier.\n",
      " |          project (Optional[str]):\n",
      " |              ID of the project which owns the job (defaults to the client's project).\n",
      " |          location (Optional[str]):\n",
      " |              Location where the job was run. Ignored if ``job_id`` is a job\n",
      " |              object.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Union[                 google.cloud.bigquery.job.LoadJob,                 google.cloud.bigquery.job.CopyJob,                 google.cloud.bigquery.job.ExtractJob,                 google.cloud.bigquery.job.QueryJob,             ]:\n",
      " |              Job instance, based on the resource returned by the API.\n",
      " |  \n",
      " |  close(self)\n",
      " |      Close the underlying transport objects, releasing system resources.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The client instance can be used for making additional requests even\n",
      " |          after closing, in which case the underlying connections are\n",
      " |          automatically re-created.\n",
      " |  \n",
      " |  copy_table(self, sources: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str, Sequence[Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str]]], destination: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], job_id: Optional[str] = None, job_id_prefix: Optional[str] = None, location: Optional[str] = None, project: Optional[str] = None, job_config: Optional[google.cloud.bigquery.job.copy_.CopyJobConfig] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.job.copy_.CopyJob\n",
      " |      Copy one or more tables to another table.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationtablecopy\n",
      " |      \n",
      " |      Args:\n",
      " |          sources (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str,                 Sequence[                     Union[                         google.cloud.bigquery.table.Table,                         google.cloud.bigquery.table.TableReference,                         google.cloud.bigquery.table.TableListItem,                         str,                     ]                 ],             ]):\n",
      " |              Table or tables to be copied.\n",
      " |          destination (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str,             ]):\n",
      " |              Table into which data is to be copied.\n",
      " |          job_id (Optional[str]): The ID of the job.\n",
      " |          job_id_prefix (Optional[str]):\n",
      " |              The user-provided prefix for a randomly generated job ID.\n",
      " |              This parameter will be ignored if a ``job_id`` is also given.\n",
      " |          location (Optional[str]):\n",
      " |              Location where to run the job. Must match the location of any\n",
      " |              source table as well as the destination table.\n",
      " |          project (Optional[str]):\n",
      " |              Project ID of the project of where to run the job. Defaults\n",
      " |              to the client's project.\n",
      " |          job_config (Optional[google.cloud.bigquery.job.CopyJobConfig]):\n",
      " |              Extra configuration options for the job.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.job.CopyJob: A new copy job instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``job_config`` is not an instance of :class:`~google.cloud.bigquery.job.CopyJobConfig`\n",
      " |              class.\n",
      " |  \n",
      " |  create_dataset(self, dataset: Union[str, google.cloud.bigquery.dataset.Dataset, google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem], exists_ok: bool = False, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.dataset.Dataset\n",
      " |      API call: create the dataset via a POST request.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/insert\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset (Union[                 google.cloud.bigquery.dataset.Dataset,                 google.cloud.bigquery.dataset.DatasetReference,                 google.cloud.bigquery.dataset.DatasetListItem,                 str,             ]):\n",
      " |              A :class:`~google.cloud.bigquery.dataset.Dataset` to create.\n",
      " |              If ``dataset`` is a reference, an empty dataset is created\n",
      " |              with the specified ID and client's default location.\n",
      " |          exists_ok (Optional[bool]):\n",
      " |              Defaults to ``False``. If ``True``, ignore \"already exists\"\n",
      " |              errors when creating the dataset.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.dataset.Dataset:\n",
      " |              A new ``Dataset`` returned from the API.\n",
      " |      \n",
      " |      Raises:\n",
      " |          google.cloud.exceptions.Conflict:\n",
      " |              If the dataset already exists.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          >>> from google.cloud import bigquery\n",
      " |          >>> client = bigquery.Client()\n",
      " |          >>> dataset = bigquery.Dataset('my_project.my_dataset')\n",
      " |          >>> dataset = client.create_dataset(dataset)\n",
      " |  \n",
      " |  create_job(self, job_config: dict, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> Union[google.cloud.bigquery.job.load.LoadJob, google.cloud.bigquery.job.copy_.CopyJob, google.cloud.bigquery.job.extract.ExtractJob, google.cloud.bigquery.job.query.QueryJob]\n",
      " |      Create a new job.\n",
      " |      \n",
      " |      Args:\n",
      " |          job_config (dict): configuration job representation returned from the API.\n",
      " |          retry (Optional[google.api_core.retry.Retry]): How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Union[                 google.cloud.bigquery.job.LoadJob,                 google.cloud.bigquery.job.CopyJob,                 google.cloud.bigquery.job.ExtractJob,                 google.cloud.bigquery.job.QueryJob             ]:\n",
      " |              A new job instance.\n",
      " |  \n",
      " |  create_routine(self, routine: google.cloud.bigquery.routine.routine.Routine, exists_ok: bool = False, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.routine.routine.Routine\n",
      " |      [Beta] Create a routine via a POST request.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/routines/insert\n",
      " |      \n",
      " |      Args:\n",
      " |          routine (google.cloud.bigquery.routine.Routine):\n",
      " |              A :class:`~google.cloud.bigquery.routine.Routine` to create.\n",
      " |              The dataset that the routine belongs to must already exist.\n",
      " |          exists_ok (Optional[bool]):\n",
      " |              Defaults to ``False``. If ``True``, ignore \"already exists\"\n",
      " |              errors when creating the routine.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.routine.Routine:\n",
      " |              A new ``Routine`` returned from the service.\n",
      " |      \n",
      " |      Raises:\n",
      " |          google.cloud.exceptions.Conflict:\n",
      " |              If the routine already exists.\n",
      " |  \n",
      " |  create_table(self, table: Union[str, google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem], exists_ok: bool = False, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.table.Table\n",
      " |      API call:  create a table via a PUT request\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/insert\n",
      " |      \n",
      " |      Args:\n",
      " |          table (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str,             ]):\n",
      " |              A :class:`~google.cloud.bigquery.table.Table` to create.\n",
      " |              If ``table`` is a reference, an empty table is created\n",
      " |              with the specified ID. The dataset that the table belongs to\n",
      " |              must already exist.\n",
      " |          exists_ok (Optional[bool]):\n",
      " |              Defaults to ``False``. If ``True``, ignore \"already exists\"\n",
      " |              errors when creating the table.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.table.Table:\n",
      " |              A new ``Table`` returned from the service.\n",
      " |      \n",
      " |      Raises:\n",
      " |          google.cloud.exceptions.Conflict:\n",
      " |              If the table already exists.\n",
      " |  \n",
      " |  dataset(self, dataset_id: str, project: Optional[str] = None) -> google.cloud.bigquery.dataset.DatasetReference\n",
      " |      Deprecated: Construct a reference to a dataset.\n",
      " |      \n",
      " |      .. deprecated:: 1.24.0\n",
      " |         Construct a\n",
      " |         :class:`~google.cloud.bigquery.dataset.DatasetReference` using its\n",
      " |         constructor or use a string where previously a reference object\n",
      " |         was used.\n",
      " |      \n",
      " |         As of ``google-cloud-bigquery`` version 1.7.0, all client methods\n",
      " |         that take a\n",
      " |         :class:`~google.cloud.bigquery.dataset.DatasetReference` or\n",
      " |         :class:`~google.cloud.bigquery.table.TableReference` also take a\n",
      " |         string in standard SQL format, e.g. ``project.dataset_id`` or\n",
      " |         ``project.dataset_id.table_id``.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_id (str): ID of the dataset.\n",
      " |      \n",
      " |          project (Optional[str]):\n",
      " |              Project ID for the dataset (defaults to the project of the client).\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.dataset.DatasetReference:\n",
      " |              a new ``DatasetReference`` instance.\n",
      " |  \n",
      " |  delete_dataset(self, dataset: Union[google.cloud.bigquery.dataset.Dataset, google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem, str], delete_contents: bool = False, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, not_found_ok: bool = False) -> None\n",
      " |      Delete a dataset.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/delete\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset (Union[                 google.cloud.bigquery.dataset.Dataset,                 google.cloud.bigquery.dataset.DatasetReference,                 google.cloud.bigquery.dataset.DatasetListItem,                 str,             ]):\n",
      " |              A reference to the dataset to delete. If a string is passed\n",
      " |              in, this method attempts to create a dataset reference from a\n",
      " |              string using\n",
      " |              :func:`google.cloud.bigquery.dataset.DatasetReference.from_string`.\n",
      " |          delete_contents (Optional[bool]):\n",
      " |              If True, delete all the tables in the dataset. If False and\n",
      " |              the dataset contains tables, the request will fail.\n",
      " |              Default is False.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          not_found_ok (Optional[bool]):\n",
      " |              Defaults to ``False``. If ``True``, ignore \"not found\" errors\n",
      " |              when deleting the dataset.\n",
      " |  \n",
      " |  delete_job_metadata(self, job_id: Union[str, google.cloud.bigquery.job.load.LoadJob, google.cloud.bigquery.job.copy_.CopyJob, google.cloud.bigquery.job.extract.ExtractJob, google.cloud.bigquery.job.query.QueryJob], project: Optional[str] = None, location: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, not_found_ok: bool = False)\n",
      " |      [Beta] Delete job metadata from job history.\n",
      " |      \n",
      " |      Note: This does not stop a running job. Use\n",
      " |      :func:`~google.cloud.bigquery.client.Client.cancel_job` instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          job_id (Union[                 str,                 LoadJob,                 CopyJob,                 ExtractJob,                 QueryJob             ]): Job or job identifier.\n",
      " |          project (Optional[str]):\n",
      " |              ID of the project which owns the job (defaults to the client's project).\n",
      " |          location (Optional[str]):\n",
      " |              Location where the job was run. Ignored if ``job_id`` is a job\n",
      " |              object.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          not_found_ok (Optional[bool]):\n",
      " |              Defaults to ``False``. If ``True``, ignore \"not found\" errors\n",
      " |              when deleting the job.\n",
      " |  \n",
      " |  delete_model(self, model: Union[google.cloud.bigquery.model.Model, google.cloud.bigquery.model.ModelReference, str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, not_found_ok: bool = False) -> None\n",
      " |      [Beta] Delete a model\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/models/delete\n",
      " |      \n",
      " |      Args:\n",
      " |          model (Union[                 google.cloud.bigquery.model.Model,                 google.cloud.bigquery.model.ModelReference,                 str,             ]):\n",
      " |              A reference to the model to delete. If a string is passed in,\n",
      " |              this method attempts to create a model reference from a\n",
      " |              string using\n",
      " |              :func:`google.cloud.bigquery.model.ModelReference.from_string`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          not_found_ok (Optional[bool]):\n",
      " |              Defaults to ``False``. If ``True``, ignore \"not found\" errors\n",
      " |              when deleting the model.\n",
      " |  \n",
      " |  delete_routine(self, routine: Union[google.cloud.bigquery.routine.routine.Routine, google.cloud.bigquery.routine.routine.RoutineReference, str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, not_found_ok: bool = False) -> None\n",
      " |      [Beta] Delete a routine.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/routines/delete\n",
      " |      \n",
      " |      Args:\n",
      " |          routine (Union[                 google.cloud.bigquery.routine.Routine,                 google.cloud.bigquery.routine.RoutineReference,                 str,             ]):\n",
      " |              A reference to the routine to delete. If a string is passed\n",
      " |              in, this method attempts to create a routine reference from a\n",
      " |              string using\n",
      " |              :func:`google.cloud.bigquery.routine.RoutineReference.from_string`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          not_found_ok (Optional[bool]):\n",
      " |              Defaults to ``False``. If ``True``, ignore \"not found\" errors\n",
      " |              when deleting the routine.\n",
      " |  \n",
      " |  delete_table(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, not_found_ok: bool = False) -> None\n",
      " |      Delete a table\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/delete\n",
      " |      \n",
      " |      Args:\n",
      " |          table (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str,             ]):\n",
      " |              A reference to the table to delete. If a string is passed in,\n",
      " |              this method attempts to create a table reference from a\n",
      " |              string using\n",
      " |              :func:`google.cloud.bigquery.table.TableReference.from_string`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          not_found_ok (Optional[bool]):\n",
      " |              Defaults to ``False``. If ``True``, ignore \"not found\" errors\n",
      " |              when deleting the table.\n",
      " |  \n",
      " |  extract_table(self, source: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, google.cloud.bigquery.model.Model, google.cloud.bigquery.model.ModelReference, str], destination_uris: Union[str, Sequence[str]], job_id: Optional[str] = None, job_id_prefix: Optional[str] = None, location: Optional[str] = None, project: Optional[str] = None, job_config: Optional[google.cloud.bigquery.job.extract.ExtractJobConfig] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, source_type: str = 'Table') -> google.cloud.bigquery.job.extract.ExtractJob\n",
      " |      Start a job to extract a table into Cloud Storage files.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationextract\n",
      " |      \n",
      " |      Args:\n",
      " |          source (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 google.cloud.bigquery.model.Model,                 google.cloud.bigquery.model.ModelReference,                 src,             ]):\n",
      " |              Table or Model to be extracted.\n",
      " |          destination_uris (Union[str, Sequence[str]]):\n",
      " |              URIs of Cloud Storage file(s) into which table data is to be\n",
      " |              extracted; in format\n",
      " |              ``gs://<bucket_name>/<object_name_or_glob>``.\n",
      " |          job_id (Optional[str]): The ID of the job.\n",
      " |          job_id_prefix (Optional[str]):\n",
      " |              The user-provided prefix for a randomly generated job ID.\n",
      " |              This parameter will be ignored if a ``job_id`` is also given.\n",
      " |          location (Optional[str]):\n",
      " |              Location where to run the job. Must match the location of the\n",
      " |              source table.\n",
      " |          project (Optional[str]):\n",
      " |              Project ID of the project of where to run the job. Defaults\n",
      " |              to the client's project.\n",
      " |          job_config (Optional[google.cloud.bigquery.job.ExtractJobConfig]):\n",
      " |              Extra configuration options for the job.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          source_type (Optional[str]):\n",
      " |              Type of source to be extracted.``Table`` or ``Model``. Defaults to ``Table``.\n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.job.ExtractJob: A new extract job instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``job_config`` is not an instance of :class:`~google.cloud.bigquery.job.ExtractJobConfig`\n",
      " |              class.\n",
      " |          ValueError:\n",
      " |              If ``source_type`` is not among ``Table``,``Model``.\n",
      " |  \n",
      " |  get_dataset(self, dataset_ref: Union[google.cloud.bigquery.dataset.DatasetReference, str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.dataset.Dataset\n",
      " |      Fetch the dataset referenced by ``dataset_ref``\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_ref (Union[                 google.cloud.bigquery.dataset.DatasetReference,                 str,             ]):\n",
      " |              A reference to the dataset to fetch from the BigQuery API.\n",
      " |              If a string is passed in, this method attempts to create a\n",
      " |              dataset reference from a string using\n",
      " |              :func:`~google.cloud.bigquery.dataset.DatasetReference.from_string`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.dataset.Dataset:\n",
      " |              A ``Dataset`` instance.\n",
      " |  \n",
      " |  get_iam_policy(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], requested_policy_version: int = 1, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.api_core.iam.Policy\n",
      " |  \n",
      " |  get_job(self, job_id: Union[str, google.cloud.bigquery.job.load.LoadJob, google.cloud.bigquery.job.copy_.CopyJob, google.cloud.bigquery.job.extract.ExtractJob, google.cloud.bigquery.job.query.QueryJob], project: Optional[str] = None, location: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> Union[google.cloud.bigquery.job.load.LoadJob, google.cloud.bigquery.job.copy_.CopyJob, google.cloud.bigquery.job.extract.ExtractJob, google.cloud.bigquery.job.query.QueryJob, google.cloud.bigquery.job.base.UnknownJob]\n",
      " |      Fetch a job for the project associated with this client.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/get\n",
      " |      \n",
      " |      Args:\n",
      " |          job_id (Union[                 str,                 job.LoadJob,                 job.CopyJob,                 job.ExtractJob,                 job.QueryJob             ]):\n",
      " |              Job identifier.\n",
      " |          project (Optional[str]):\n",
      " |              ID of the project which owns the job (defaults to the client's project).\n",
      " |          location (Optional[str]):\n",
      " |              Location where the job was run. Ignored if ``job_id`` is a job\n",
      " |              object.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Union[job.LoadJob, job.CopyJob, job.ExtractJob, job.QueryJob, job.UnknownJob]:\n",
      " |              Job instance, based on the resource returned by the API.\n",
      " |  \n",
      " |  get_model(self, model_ref: Union[google.cloud.bigquery.model.ModelReference, str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.model.Model\n",
      " |      [Beta] Fetch the model referenced by ``model_ref``.\n",
      " |      \n",
      " |      Args:\n",
      " |         model_ref (Union[                 google.cloud.bigquery.model.ModelReference,                 str,             ]):\n",
      " |             A reference to the model to fetch from the BigQuery API.\n",
      " |             If a string is passed in, this method attempts to create a\n",
      " |             model reference from a string using\n",
      " |             :func:`google.cloud.bigquery.model.ModelReference.from_string`.\n",
      " |         retry (Optional[google.api_core.retry.Retry]):\n",
      " |             How to retry the RPC.\n",
      " |         timeout (Optional[float]):\n",
      " |             The number of seconds to wait for the underlying HTTP transport\n",
      " |             before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |         google.cloud.bigquery.model.Model: A ``Model`` instance.\n",
      " |  \n",
      " |  get_routine(self, routine_ref: Union[google.cloud.bigquery.routine.routine.Routine, google.cloud.bigquery.routine.routine.RoutineReference, str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.routine.routine.Routine\n",
      " |      [Beta] Get the routine referenced by ``routine_ref``.\n",
      " |      \n",
      " |      Args:\n",
      " |         routine_ref (Union[                 google.cloud.bigquery.routine.Routine,                 google.cloud.bigquery.routine.RoutineReference,                 str,             ]):\n",
      " |             A reference to the routine to fetch from the BigQuery API. If\n",
      " |             a string is passed in, this method attempts to create a\n",
      " |             reference from a string using\n",
      " |             :func:`google.cloud.bigquery.routine.RoutineReference.from_string`.\n",
      " |         retry (Optional[google.api_core.retry.Retry]):\n",
      " |             How to retry the API call.\n",
      " |         timeout (Optional[float]):\n",
      " |             The number of seconds to wait for the underlying HTTP transport\n",
      " |             before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |         google.cloud.bigquery.routine.Routine:\n",
      " |             A ``Routine`` instance.\n",
      " |  \n",
      " |  get_service_account_email(self, project: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> str\n",
      " |      Get the email address of the project's BigQuery service account\n",
      " |      \n",
      " |      Note:\n",
      " |          This is the service account that BigQuery uses to manage tables\n",
      " |          encrypted by a key in KMS.\n",
      " |      \n",
      " |      Args:\n",
      " |          project (Optional[str]):\n",
      " |              Project ID to use for retreiving service account email.\n",
      " |              Defaults to the client's project.\n",
      " |          retry (Optional[google.api_core.retry.Retry]): How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str:\n",
      " |              service account email address\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          >>> from google.cloud import bigquery\n",
      " |          >>> client = bigquery.Client()\n",
      " |          >>> client.get_service_account_email()\n",
      " |          my_service_account@my-project.iam.gserviceaccount.com\n",
      " |  \n",
      " |  get_table(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.table.Table\n",
      " |      Fetch the table referenced by ``table``.\n",
      " |      \n",
      " |      Args:\n",
      " |          table (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str,             ]):\n",
      " |              A reference to the table to fetch from the BigQuery API.\n",
      " |              If a string is passed in, this method attempts to create a\n",
      " |              table reference from a string using\n",
      " |              :func:`google.cloud.bigquery.table.TableReference.from_string`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.table.Table:\n",
      " |              A ``Table`` instance.\n",
      " |  \n",
      " |  insert_rows(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, str], rows: Union[Iterable[Tuple], Iterable[Mapping[str, Any]]], selected_fields: Optional[Sequence[google.cloud.bigquery.schema.SchemaField]] = None, **kwargs) -> Sequence[Dict[str, Any]]\n",
      " |      Insert rows into a table via the streaming API.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll\n",
      " |      \n",
      " |      BigQuery will reject insertAll payloads that exceed a defined limit (10MB).\n",
      " |      Additionally, if a payload vastly exceeds this limit, the request is rejected\n",
      " |      by the intermediate architecture, which returns a 413 (Payload Too Large) status code.\n",
      " |      \n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/quotas#streaming_inserts\n",
      " |      \n",
      " |      Args:\n",
      " |          table (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 str,             ]):\n",
      " |              The destination table for the row data, or a reference to it.\n",
      " |          rows (Union[Sequence[Tuple], Sequence[Dict]]):\n",
      " |              Row data to be inserted. If a list of tuples is given, each\n",
      " |              tuple should contain data for each schema field on the\n",
      " |              current table and in the same order as the schema fields. If\n",
      " |              a list of dictionaries is given, the keys must include all\n",
      " |              required fields in the schema. Keys which do not correspond\n",
      " |              to a field in the schema are ignored.\n",
      " |          selected_fields (Sequence[google.cloud.bigquery.schema.SchemaField]):\n",
      " |              The fields to return. Required if ``table`` is a\n",
      " |              :class:`~google.cloud.bigquery.table.TableReference`.\n",
      " |          kwargs (dict):\n",
      " |              Keyword arguments to\n",
      " |              :meth:`~google.cloud.bigquery.client.Client.insert_rows_json`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Sequence[Mappings]:\n",
      " |              One mapping per row with insert errors: the \"index\" key\n",
      " |              identifies the row, and the \"errors\" key contains a list of\n",
      " |              the mappings describing one or more problems with the row.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if table's schema is not set or `rows` is not a `Sequence`.\n",
      " |  \n",
      " |  insert_rows_from_dataframe(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, str], dataframe, selected_fields: Optional[Sequence[google.cloud.bigquery.schema.SchemaField]] = None, chunk_size: int = 500, **kwargs: Dict) -> Sequence[Sequence[dict]]\n",
      " |      Insert rows into a table from a dataframe via the streaming API.\n",
      " |      \n",
      " |      BigQuery will reject insertAll payloads that exceed a defined limit (10MB).\n",
      " |      Additionally, if a payload vastly exceeds this limit, the request is rejected\n",
      " |      by the intermediate architecture, which returns a 413 (Payload Too Large) status code.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/quotas#streaming_inserts\n",
      " |      \n",
      " |      Args:\n",
      " |          table (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 str,             ]):\n",
      " |              The destination table for the row data, or a reference to it.\n",
      " |          dataframe (pandas.DataFrame):\n",
      " |              A :class:`~pandas.DataFrame` containing the data to load. Any\n",
      " |              ``NaN`` values present in the dataframe are omitted from the\n",
      " |              streaming API request(s).\n",
      " |          selected_fields (Sequence[google.cloud.bigquery.schema.SchemaField]):\n",
      " |              The fields to return. Required if ``table`` is a\n",
      " |              :class:`~google.cloud.bigquery.table.TableReference`.\n",
      " |          chunk_size (int):\n",
      " |              The number of rows to stream in a single chunk. Must be positive.\n",
      " |          kwargs (Dict):\n",
      " |              Keyword arguments to\n",
      " |              :meth:`~google.cloud.bigquery.client.Client.insert_rows_json`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Sequence[Sequence[Mappings]]:\n",
      " |              A list with insert errors for each insert chunk. Each element\n",
      " |              is a list containing one mapping per row with insert errors:\n",
      " |              the \"index\" key identifies the row, and the \"errors\" key\n",
      " |              contains a list of the mappings describing one or more problems\n",
      " |              with the row.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if table's schema is not set\n",
      " |  \n",
      " |  insert_rows_json(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], json_rows: Sequence[Mapping[str, Any]], row_ids: Union[Iterable[Optional[str]], google.cloud.bigquery.enums.AutoRowIDs, NoneType] = <AutoRowIDs.GENERATE_UUID: 2>, skip_invalid_rows: Optional[bool] = None, ignore_unknown_values: Optional[bool] = None, template_suffix: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> Sequence[dict]\n",
      " |      Insert rows into a table without applying local type conversions.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll\n",
      " |      \n",
      " |      BigQuery will reject insertAll payloads that exceed a defined limit (10MB).\n",
      " |      Additionally, if a payload vastly exceeds this limit, the request is rejected\n",
      " |      by the intermediate architecture, which returns a 413 (Payload Too Large) status code.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/quotas#streaming_inserts\n",
      " |      \n",
      " |      Args:\n",
      " |          table (Union[                 google.cloud.bigquery.table.Table                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str             ]):\n",
      " |              The destination table for the row data, or a reference to it.\n",
      " |          json_rows (Sequence[Dict]):\n",
      " |              Row data to be inserted. Keys must match the table schema fields\n",
      " |              and values must be JSON-compatible representations.\n",
      " |          row_ids (Union[Iterable[str], AutoRowIDs, None]):\n",
      " |              Unique IDs, one per row being inserted. An ID can also be\n",
      " |              ``None``, indicating that an explicit insert ID should **not**\n",
      " |              be used for that row. If the argument is omitted altogether,\n",
      " |              unique IDs are created automatically.\n",
      " |      \n",
      " |              .. versionchanged:: 2.21.0\n",
      " |                  Can also be an iterable, not just a sequence, or an\n",
      " |                  :class:`AutoRowIDs` enum member.\n",
      " |      \n",
      " |              .. deprecated:: 2.21.0\n",
      " |                  Passing ``None`` to explicitly request autogenerating insert IDs is\n",
      " |                  deprecated, use :attr:`AutoRowIDs.GENERATE_UUID` instead.\n",
      " |      \n",
      " |          skip_invalid_rows (Optional[bool]):\n",
      " |              Insert all valid rows of a request, even if invalid rows exist.\n",
      " |              The default value is ``False``, which causes the entire request\n",
      " |              to fail if any invalid rows exist.\n",
      " |          ignore_unknown_values (Optional[bool]):\n",
      " |              Accept rows that contain values that do not match the schema.\n",
      " |              The unknown values are ignored. Default is ``False``, which\n",
      " |              treats unknown values as errors.\n",
      " |          template_suffix (Optional[str]):\n",
      " |              Treat ``name`` as a template table and provide a suffix.\n",
      " |              BigQuery will create the table ``<name> + <template_suffix>``\n",
      " |              based on the schema of the template table. See\n",
      " |              https://cloud.google.com/bigquery/streaming-data-into-bigquery#template-tables\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Sequence[Mappings]:\n",
      " |              One mapping per row with insert errors: the \"index\" key\n",
      " |              identifies the row, and the \"errors\" key contains a list of\n",
      " |              the mappings describing one or more problems with the row.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError: if `json_rows` is not a `Sequence`.\n",
      " |  \n",
      " |  job_from_resource(self, resource: dict) -> Union[google.cloud.bigquery.job.copy_.CopyJob, google.cloud.bigquery.job.extract.ExtractJob, google.cloud.bigquery.job.load.LoadJob, google.cloud.bigquery.job.query.QueryJob, google.cloud.bigquery.job.base.UnknownJob]\n",
      " |      Detect correct job type from resource and instantiate.\n",
      " |      \n",
      " |      Args:\n",
      " |          resource (Dict): one job resource from API response\n",
      " |      \n",
      " |      Returns:\n",
      " |          Union[job.CopyJob, job.ExtractJob, job.LoadJob, job.QueryJob, job.UnknownJob]:\n",
      " |              The job instance, constructed via the resource.\n",
      " |  \n",
      " |  list_datasets(self, project: Optional[str] = None, include_all: bool = False, filter: Optional[str] = None, max_results: Optional[int] = None, page_token: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, page_size: Optional[int] = None) -> google.api_core.page_iterator.Iterator\n",
      " |      List datasets for the project associated with this client.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list\n",
      " |      \n",
      " |      Args:\n",
      " |          project (Optional[str]):\n",
      " |              Project ID to use for retreiving datasets. Defaults to the\n",
      " |              client's project.\n",
      " |          include_all (Optional[bool]):\n",
      " |              True if results include hidden datasets. Defaults to False.\n",
      " |          filter (Optional[str]):\n",
      " |              An expression for filtering the results by label.\n",
      " |              For syntax, see\n",
      " |              https://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/list#body.QUERY_PARAMETERS.filter\n",
      " |          max_results (Optional[int]):\n",
      " |              Maximum number of datasets to return.\n",
      " |          page_token (Optional[str]):\n",
      " |              Token representing a cursor into the datasets. If not passed,\n",
      " |              the API will return the first page of datasets. The token marks\n",
      " |              the beginning of the iterator to be returned and the value of\n",
      " |              the ``page_token`` can be accessed at ``next_page_token`` of the\n",
      " |              :class:`~google.api_core.page_iterator.HTTPIterator`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          page_size (Optional[int]):\n",
      " |              Maximum number of datasets to return per page.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.api_core.page_iterator.Iterator:\n",
      " |              Iterator of :class:`~google.cloud.bigquery.dataset.DatasetListItem`.\n",
      " |              associated with the project.\n",
      " |  \n",
      " |  list_jobs(self, project: Optional[str] = None, parent_job: Union[google.cloud.bigquery.job.query.QueryJob, str, NoneType] = None, max_results: Optional[int] = None, page_token: Optional[str] = None, all_users: Optional[bool] = None, state_filter: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, min_creation_time: Optional[datetime.datetime] = None, max_creation_time: Optional[datetime.datetime] = None, page_size: Optional[int] = None) -> google.api_core.page_iterator.Iterator\n",
      " |      List jobs for the project associated with this client.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/list\n",
      " |      \n",
      " |      Args:\n",
      " |          project (Optional[str]):\n",
      " |              Project ID to use for retreiving datasets. Defaults\n",
      " |              to the client's project.\n",
      " |          parent_job (Optional[Union[                 google.cloud.bigquery.job._AsyncJob,                 str,             ]]):\n",
      " |              If set, retrieve only child jobs of the specified parent.\n",
      " |          max_results (Optional[int]):\n",
      " |              Maximum number of jobs to return.\n",
      " |          page_token (Optional[str]):\n",
      " |              Opaque marker for the next \"page\" of jobs. If not\n",
      " |              passed, the API will return the first page of jobs. The token\n",
      " |              marks the beginning of the iterator to be returned and the\n",
      " |              value of the ``page_token`` can be accessed at\n",
      " |              ``next_page_token`` of\n",
      " |              :class:`~google.api_core.page_iterator.HTTPIterator`.\n",
      " |          all_users (Optional[bool]):\n",
      " |              If true, include jobs owned by all users in the project.\n",
      " |              Defaults to :data:`False`.\n",
      " |          state_filter (Optional[str]):\n",
      " |              If set, include only jobs matching the given state. One of:\n",
      " |                  * ``\"done\"``\n",
      " |                  * ``\"pending\"``\n",
      " |                  * ``\"running\"``\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          min_creation_time (Optional[datetime.datetime]):\n",
      " |              Min value for job creation time. If set, only jobs created\n",
      " |              after or at this timestamp are returned. If the datetime has\n",
      " |              no time zone assumes UTC time.\n",
      " |          max_creation_time (Optional[datetime.datetime]):\n",
      " |              Max value for job creation time. If set, only jobs created\n",
      " |              before or at this timestamp are returned. If the datetime has\n",
      " |              no time zone assumes UTC time.\n",
      " |          page_size (Optional[int]):\n",
      " |              Maximum number of jobs to return per page.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.api_core.page_iterator.Iterator:\n",
      " |              Iterable of job instances.\n",
      " |  \n",
      " |  list_models(self, dataset: Union[google.cloud.bigquery.dataset.Dataset, google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem, str], max_results: Optional[int] = None, page_token: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, page_size: Optional[int] = None) -> google.api_core.page_iterator.Iterator\n",
      " |      [Beta] List models in the dataset.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/models/list\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset (Union[                 google.cloud.bigquery.dataset.Dataset,                 google.cloud.bigquery.dataset.DatasetReference,                 google.cloud.bigquery.dataset.DatasetListItem,                 str,             ]):\n",
      " |              A reference to the dataset whose models to list from the\n",
      " |              BigQuery API. If a string is passed in, this method attempts\n",
      " |              to create a dataset reference from a string using\n",
      " |              :func:`google.cloud.bigquery.dataset.DatasetReference.from_string`.\n",
      " |          max_results (Optional[int]):\n",
      " |              Maximum number of models to return. Defaults to a\n",
      " |              value set by the API.\n",
      " |          page_token (Optional[str]):\n",
      " |              Token representing a cursor into the models. If not passed,\n",
      " |              the API will return the first page of models. The token marks\n",
      " |              the beginning of the iterator to be returned and the value of\n",
      " |              the ``page_token`` can be accessed at ``next_page_token`` of the\n",
      " |              :class:`~google.api_core.page_iterator.HTTPIterator`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          page_size (Optional[int]):\n",
      " |              Maximum number of models to return per page.\n",
      " |              Defaults to a value set by the API.\n",
      " |      \n",
      " |       Returns:\n",
      " |          google.api_core.page_iterator.Iterator:\n",
      " |              Iterator of\n",
      " |              :class:`~google.cloud.bigquery.model.Model` contained\n",
      " |              within the requested dataset.\n",
      " |  \n",
      " |  list_partitions(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> Sequence[str]\n",
      " |      List the partitions in a table.\n",
      " |      \n",
      " |      Args:\n",
      " |          table (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str,             ]):\n",
      " |              The table or reference from which to get partition info\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |              If multiple requests are made under the hood, ``timeout``\n",
      " |              applies to each individual request.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]:\n",
      " |              A list of the partition ids present in the partitioned table\n",
      " |  \n",
      " |  list_projects(self, max_results: Optional[int] = None, page_token: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, page_size: Optional[int] = None) -> google.api_core.page_iterator.Iterator\n",
      " |      List projects for the project associated with this client.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/projects/list\n",
      " |      \n",
      " |      Args:\n",
      " |          max_results (Optional[int]):\n",
      " |              Maximum number of projects to return.\n",
      " |              Defaults to a value set by the API.\n",
      " |      \n",
      " |          page_token (Optional[str]):\n",
      " |              Token representing a cursor into the projects. If not passed,\n",
      " |              the API will return the first page of projects. The token marks\n",
      " |              the beginning of the iterator to be returned and the value of\n",
      " |              the ``page_token`` can be accessed at ``next_page_token`` of the\n",
      " |              :class:`~google.api_core.page_iterator.HTTPIterator`.\n",
      " |      \n",
      " |          retry (Optional[google.api_core.retry.Retry]): How to retry the RPC.\n",
      " |      \n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |          page_size (Optional[int]):\n",
      " |              Maximum number of projects to return in each page.\n",
      " |              Defaults to a value set by the API.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.api_core.page_iterator.Iterator:\n",
      " |              Iterator of :class:`~google.cloud.bigquery.client.Project`\n",
      " |              accessible to the current client.\n",
      " |  \n",
      " |  list_routines(self, dataset: Union[google.cloud.bigquery.dataset.Dataset, google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem, str], max_results: Optional[int] = None, page_token: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, page_size: Optional[int] = None) -> google.api_core.page_iterator.Iterator\n",
      " |      [Beta] List routines in the dataset.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/routines/list\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset (Union[                 google.cloud.bigquery.dataset.Dataset,                 google.cloud.bigquery.dataset.DatasetReference,                 google.cloud.bigquery.dataset.DatasetListItem,                 str,             ]):\n",
      " |              A reference to the dataset whose routines to list from the\n",
      " |              BigQuery API. If a string is passed in, this method attempts\n",
      " |              to create a dataset reference from a string using\n",
      " |              :func:`google.cloud.bigquery.dataset.DatasetReference.from_string`.\n",
      " |          max_results (Optional[int]):\n",
      " |              Maximum number of routines to return. Defaults\n",
      " |              to a value set by the API.\n",
      " |          page_token (Optional[str]):\n",
      " |              Token representing a cursor into the routines. If not passed,\n",
      " |              the API will return the first page of routines. The token marks\n",
      " |              the beginning of the iterator to be returned and the value of the\n",
      " |              ``page_token`` can be accessed at ``next_page_token`` of the\n",
      " |              :class:`~google.api_core.page_iterator.HTTPIterator`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          page_size (Optional[int]):\n",
      " |              Maximum number of routines to return per page.\n",
      " |              Defaults to a value set by the API.\n",
      " |      \n",
      " |       Returns:\n",
      " |          google.api_core.page_iterator.Iterator:\n",
      " |              Iterator of all\n",
      " |              :class:`~google.cloud.bigquery.routine.Routine`s contained\n",
      " |              within the requested dataset, limited by ``max_results``.\n",
      " |  \n",
      " |  list_rows(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableListItem, google.cloud.bigquery.table.TableReference, str], selected_fields: Optional[Sequence[google.cloud.bigquery.schema.SchemaField]] = None, max_results: Optional[int] = None, page_token: Optional[str] = None, start_index: Optional[int] = None, page_size: Optional[int] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.table.RowIterator\n",
      " |      List the rows of the table.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/list\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         This method assumes that the provided schema is up-to-date with the\n",
      " |         schema as defined on the back-end: if the two schemas are not\n",
      " |         identical, the values returned may be incomplete. To ensure that the\n",
      " |         local copy of the schema is up-to-date, call ``client.get_table``.\n",
      " |      \n",
      " |      Args:\n",
      " |          table (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableListItem,                 google.cloud.bigquery.table.TableReference,                 str,             ]):\n",
      " |              The table to list, or a reference to it. When the table\n",
      " |              object does not contain a schema and ``selected_fields`` is\n",
      " |              not supplied, this method calls ``get_table`` to fetch the\n",
      " |              table schema.\n",
      " |          selected_fields (Sequence[google.cloud.bigquery.schema.SchemaField]):\n",
      " |              The fields to return. If not supplied, data for all columns\n",
      " |              are downloaded.\n",
      " |          max_results (Optional[int]):\n",
      " |              Maximum number of rows to return.\n",
      " |          page_token (Optional[str]):\n",
      " |              Token representing a cursor into the table's rows.\n",
      " |              If not passed, the API will return the first page of the\n",
      " |              rows. The token marks the beginning of the iterator to be\n",
      " |              returned and the value of the ``page_token`` can be accessed\n",
      " |              at ``next_page_token`` of the\n",
      " |              :class:`~google.cloud.bigquery.table.RowIterator`.\n",
      " |          start_index (Optional[int]):\n",
      " |              The zero-based index of the starting row to read.\n",
      " |          page_size (Optional[int]):\n",
      " |              The maximum number of rows in each page of results from this request.\n",
      " |              Non-positive values are ignored. Defaults to a sensible value set by the API.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |              If multiple requests are made under the hood, ``timeout``\n",
      " |              applies to each individual request.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.table.RowIterator:\n",
      " |              Iterator of row data\n",
      " |              :class:`~google.cloud.bigquery.table.Row`-s. During each\n",
      " |              page, the iterator will have the ``total_rows`` attribute\n",
      " |              set, which counts the total number of rows **in the table**\n",
      " |              (this is distinct from the total number of rows in the\n",
      " |              current page: ``iterator.page.num_items``).\n",
      " |  \n",
      " |  list_tables(self, dataset: Union[google.cloud.bigquery.dataset.Dataset, google.cloud.bigquery.dataset.DatasetReference, google.cloud.bigquery.dataset.DatasetListItem, str], max_results: Optional[int] = None, page_token: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, page_size: Optional[int] = None) -> google.api_core.page_iterator.Iterator\n",
      " |      List tables in the dataset.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/tables/list\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset (Union[                 google.cloud.bigquery.dataset.Dataset,                 google.cloud.bigquery.dataset.DatasetReference,                 google.cloud.bigquery.dataset.DatasetListItem,                 str,             ]):\n",
      " |              A reference to the dataset whose tables to list from the\n",
      " |              BigQuery API. If a string is passed in, this method attempts\n",
      " |              to create a dataset reference from a string using\n",
      " |              :func:`google.cloud.bigquery.dataset.DatasetReference.from_string`.\n",
      " |          max_results (Optional[int]):\n",
      " |              Maximum number of tables to return. Defaults\n",
      " |              to a value set by the API.\n",
      " |          page_token (Optional[str]):\n",
      " |              Token representing a cursor into the tables. If not passed,\n",
      " |              the API will return the first page of tables. The token marks\n",
      " |              the beginning of the iterator to be returned and the value of\n",
      " |              the ``page_token`` can be accessed at ``next_page_token`` of the\n",
      " |              :class:`~google.api_core.page_iterator.HTTPIterator`.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          page_size (Optional[int]):\n",
      " |              Maximum number of tables to return per page.\n",
      " |              Defaults to a value set by the API.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.api_core.page_iterator.Iterator:\n",
      " |              Iterator of\n",
      " |              :class:`~google.cloud.bigquery.table.TableListItem` contained\n",
      " |              within the requested dataset.\n",
      " |  \n",
      " |  load_table_from_dataframe(self, dataframe: 'pandas.DataFrame', destination: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, str], num_retries: int = 6, job_id: Optional[str] = None, job_id_prefix: Optional[str] = None, location: Optional[str] = None, project: Optional[str] = None, job_config: Optional[google.cloud.bigquery.job.load.LoadJobConfig] = None, parquet_compression: str = 'snappy', timeout: Union[NoneType, float, Tuple[float, float]] = None) -> google.cloud.bigquery.job.load.LoadJob\n",
      " |      Upload the contents of a table from a pandas DataFrame.\n",
      " |      \n",
      " |      Similar to :meth:`load_table_from_uri`, this method creates, starts and\n",
      " |      returns a :class:`~google.cloud.bigquery.job.LoadJob`.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          REPEATED fields are NOT supported when using the CSV source format.\n",
      " |          They are supported when using the PARQUET source format, but\n",
      " |          due to the way they are encoded in the ``parquet`` file,\n",
      " |          a mismatch with the existing table schema can occur, so\n",
      " |          REPEATED fields are not properly supported when using ``pyarrow<4.0.0``\n",
      " |          using the parquet format.\n",
      " |      \n",
      " |          https://github.com/googleapis/python-bigquery/issues/19\n",
      " |      \n",
      " |      Args:\n",
      " |          dataframe (pandas.Dataframe):\n",
      " |              A :class:`~pandas.DataFrame` containing the data to load.\n",
      " |          destination (Union[                 Table,                 TableReference,                 str             ]):\n",
      " |              The destination table to use for loading the data. If it is an\n",
      " |              existing table, the schema of the :class:`~pandas.DataFrame`\n",
      " |              must match the schema of the destination table. If the table\n",
      " |              does not yet exist, the schema is inferred from the\n",
      " |              :class:`~pandas.DataFrame`.\n",
      " |      \n",
      " |              If a string is passed in, this method attempts to create a\n",
      " |              table reference from a string using\n",
      " |              :func:`google.cloud.bigquery.table.TableReference.from_string`.\n",
      " |          num_retries (Optional[int]): Number of upload retries. Defaults to 6.\n",
      " |          job_id (Optional[str]): Name of the job.\n",
      " |          job_id_prefix (Optional[str]):\n",
      " |              The user-provided prefix for a randomly generated\n",
      " |              job ID. This parameter will be ignored if a ``job_id`` is\n",
      " |              also given.\n",
      " |          location (Optional[str]):\n",
      " |              Location where to run the job. Must match the location of the\n",
      " |              destination table.\n",
      " |          project (Optional[str]):\n",
      " |              Project ID of the project of where to run the job. Defaults\n",
      " |              to the client's project.\n",
      " |          job_config (Optional[LoadJobConfig]):\n",
      " |              Extra configuration options for the job.\n",
      " |      \n",
      " |              To override the default pandas data type conversions, supply\n",
      " |              a value for\n",
      " |              :attr:`~google.cloud.bigquery.job.LoadJobConfig.schema` with\n",
      " |              column names matching those of the dataframe. The BigQuery\n",
      " |              schema is used to determine the correct data type conversion.\n",
      " |              Indexes are not loaded.\n",
      " |      \n",
      " |              By default, this method uses the parquet source format. To\n",
      " |              override this, supply a value for\n",
      " |              :attr:`~google.cloud.bigquery.job.LoadJobConfig.source_format`\n",
      " |              with the format name. Currently only\n",
      " |              :attr:`~google.cloud.bigquery.job.SourceFormat.CSV` and\n",
      " |              :attr:`~google.cloud.bigquery.job.SourceFormat.PARQUET` are\n",
      " |              supported.\n",
      " |          parquet_compression (Optional[str]):\n",
      " |              [Beta] The compression method to use if intermittently\n",
      " |              serializing ``dataframe`` to a parquet file.\n",
      " |              Defaults to \"snappy\".\n",
      " |      \n",
      " |              The argument is directly passed as the ``compression``\n",
      " |              argument to the underlying ``pyarrow.parquet.write_table()``\n",
      " |              method (the default value \"snappy\" gets converted to uppercase).\n",
      " |              https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_table.html#pyarrow-parquet-write-table\n",
      " |      \n",
      " |              If the job config schema is missing, the argument is directly\n",
      " |              passed as the ``compression`` argument to the underlying\n",
      " |              ``DataFrame.to_parquet()`` method.\n",
      " |              https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html#pandas.DataFrame.to_parquet\n",
      " |          timeout (Optional[flaot]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``. Depending on the retry strategy, a request may\n",
      " |              be repeated several times using the same timeout each time.\n",
      " |              Defaults to None.\n",
      " |      \n",
      " |              Can also be passed as a tuple (connect_timeout, read_timeout).\n",
      " |              See :meth:`requests.Session.request` documentation for details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.job.LoadJob: A new load job.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError:\n",
      " |              If a usable parquet engine cannot be found. This method\n",
      " |              requires :mod:`pyarrow` to be installed.\n",
      " |          TypeError:\n",
      " |              If ``job_config`` is not an instance of\n",
      " |              :class:`~google.cloud.bigquery.job.LoadJobConfig` class.\n",
      " |  \n",
      " |  load_table_from_file(self, file_obj: IO[bytes], destination: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], rewind: bool = False, size: Optional[int] = None, num_retries: int = 6, job_id: Optional[str] = None, job_id_prefix: Optional[str] = None, location: Optional[str] = None, project: Optional[str] = None, job_config: Optional[google.cloud.bigquery.job.load.LoadJobConfig] = None, timeout: Union[NoneType, float, Tuple[float, float]] = None) -> google.cloud.bigquery.job.load.LoadJob\n",
      " |      Upload the contents of this table from a file-like object.\n",
      " |      \n",
      " |      Similar to :meth:`load_table_from_uri`, this method creates, starts and\n",
      " |      returns a :class:`~google.cloud.bigquery.job.LoadJob`.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_obj (IO[bytes]):\n",
      " |              A file handle opened in binary mode for reading.\n",
      " |          destination (Union[Table,                 TableReference,                 TableListItem,                 str             ]):\n",
      " |              Table into which data is to be loaded. If a string is passed\n",
      " |              in, this method attempts to create a table reference from a\n",
      " |              string using\n",
      " |              :func:`google.cloud.bigquery.table.TableReference.from_string`.\n",
      " |          rewind (Optional[bool]):\n",
      " |              If True, seek to the beginning of the file handle before\n",
      " |              reading the file. Defaults to False.\n",
      " |          size (Optional[int]):\n",
      " |              The number of bytes to read from the file handle. If size is\n",
      " |              ``None`` or large, resumable upload will be used. Otherwise,\n",
      " |              multipart upload will be used.\n",
      " |          num_retries (Optional[int]): Number of upload retries. Defaults to 6.\n",
      " |          job_id (Optional[str]): Name of the job.\n",
      " |          job_id_prefix (Optional[str]):\n",
      " |              The user-provided prefix for a randomly generated job ID.\n",
      " |              This parameter will be ignored if a ``job_id`` is also given.\n",
      " |          location (Optional[str]):\n",
      " |              Location where to run the job. Must match the location of the\n",
      " |              destination table.\n",
      " |          project (Optional[str]):\n",
      " |              Project ID of the project of where to run the job. Defaults\n",
      " |              to the client's project.\n",
      " |          job_config (Optional[LoadJobConfig]):\n",
      " |              Extra configuration options for the job.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``. Depending on the retry strategy, a request\n",
      " |              may be repeated several times using the same timeout each time.\n",
      " |              Defaults to None.\n",
      " |      \n",
      " |              Can also be passed as a tuple (connect_timeout, read_timeout).\n",
      " |              See :meth:`requests.Session.request` documentation for details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.job.LoadJob: A new load job.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError:\n",
      " |              If ``size`` is not passed in and can not be determined, or if\n",
      " |              the ``file_obj`` can be detected to be a file opened in text\n",
      " |              mode.\n",
      " |      \n",
      " |          TypeError:\n",
      " |              If ``job_config`` is not an instance of\n",
      " |              :class:`~google.cloud.bigquery.job.LoadJobConfig` class.\n",
      " |  \n",
      " |  load_table_from_json(self, json_rows: Iterable[Dict[str, Any]], destination: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], num_retries: int = 6, job_id: Optional[str] = None, job_id_prefix: Optional[str] = None, location: Optional[str] = None, project: Optional[str] = None, job_config: Optional[google.cloud.bigquery.job.load.LoadJobConfig] = None, timeout: Union[NoneType, float, Tuple[float, float]] = None) -> google.cloud.bigquery.job.load.LoadJob\n",
      " |      Upload the contents of a table from a JSON string or dict.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_rows (Iterable[Dict[str, Any]]):\n",
      " |              Row data to be inserted. Keys must match the table schema fields\n",
      " |              and values must be JSON-compatible representations.\n",
      " |      \n",
      " |              .. note::\n",
      " |      \n",
      " |                  If your data is already a newline-delimited JSON string,\n",
      " |                  it is best to wrap it into a file-like object and pass it\n",
      " |                  to :meth:`~google.cloud.bigquery.client.Client.load_table_from_file`::\n",
      " |      \n",
      " |                      import io\n",
      " |                      from google.cloud import bigquery\n",
      " |      \n",
      " |                      data = u'{\"foo\": \"bar\"}'\n",
      " |                      data_as_file = io.StringIO(data)\n",
      " |      \n",
      " |                      client = bigquery.Client()\n",
      " |                      client.load_table_from_file(data_as_file, ...)\n",
      " |      \n",
      " |          destination (Union[                 Table,                 TableReference,                 TableListItem,                 str             ]):\n",
      " |              Table into which data is to be loaded. If a string is passed\n",
      " |              in, this method attempts to create a table reference from a\n",
      " |              string using\n",
      " |              :func:`google.cloud.bigquery.table.TableReference.from_string`.\n",
      " |          num_retries (Optional[int]): Number of upload retries. Defaults to 6.\n",
      " |          job_id (Optional[str]): Name of the job.\n",
      " |          job_id_prefix (Optional[str]):\n",
      " |              The user-provided prefix for a randomly generated job ID.\n",
      " |              This parameter will be ignored if a ``job_id`` is also given.\n",
      " |          location (Optional[str]):\n",
      " |              Location where to run the job. Must match the location of the\n",
      " |              destination table.\n",
      " |          project (Optional[str]):\n",
      " |              Project ID of the project of where to run the job. Defaults\n",
      " |              to the client's project.\n",
      " |          job_config (Optional[LoadJobConfig]):\n",
      " |              Extra configuration options for the job. The ``source_format``\n",
      " |              setting is always set to\n",
      " |              :attr:`~google.cloud.bigquery.job.SourceFormat.NEWLINE_DELIMITED_JSON`.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``. Depending on the retry strategy, a request may\n",
      " |              be repeated several times using the same timeout each time.\n",
      " |              Defaults to None.\n",
      " |      \n",
      " |              Can also be passed as a tuple (connect_timeout, read_timeout).\n",
      " |              See :meth:`requests.Session.request` documentation for details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.job.LoadJob: A new load job.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``job_config`` is not an instance of\n",
      " |              :class:`~google.cloud.bigquery.job.LoadJobConfig` class.\n",
      " |  \n",
      " |  load_table_from_uri(self, source_uris: Union[str, Sequence[str]], destination: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], job_id: Optional[str] = None, job_id_prefix: Optional[str] = None, location: Optional[str] = None, project: Optional[str] = None, job_config: Optional[google.cloud.bigquery.job.load.LoadJobConfig] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.job.load.LoadJob\n",
      " |      Starts a job for loading data into a table from Cloud Storage.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationload\n",
      " |      \n",
      " |      Args:\n",
      " |          source_uris (Union[str, Sequence[str]]):\n",
      " |              URIs of data files to be loaded; in format\n",
      " |              ``gs://<bucket_name>/<object_name_or_glob>``.\n",
      " |          destination (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str,             ]):\n",
      " |              Table into which data is to be loaded. If a string is passed\n",
      " |              in, this method attempts to create a table reference from a\n",
      " |              string using\n",
      " |              :func:`google.cloud.bigquery.table.TableReference.from_string`.\n",
      " |          job_id (Optional[str]): Name of the job.\n",
      " |          job_id_prefix (Optional[str]):\n",
      " |              The user-provided prefix for a randomly generated job ID.\n",
      " |              This parameter will be ignored if a ``job_id`` is also given.\n",
      " |          location (Optional[str]):\n",
      " |              Location where to run the job. Must match the location of the\n",
      " |              destination table.\n",
      " |          project (Optional[str]):\n",
      " |              Project ID of the project of where to run the job. Defaults\n",
      " |              to the client's project.\n",
      " |          job_config (Optional[google.cloud.bigquery.job.LoadJobConfig]):\n",
      " |              Extra configuration options for the job.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.job.LoadJob: A new load job.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``job_config`` is not an instance of\n",
      " |              :class:`~google.cloud.bigquery.job.LoadJobConfig` class.\n",
      " |  \n",
      " |  query(self, query: str, job_config: Optional[google.cloud.bigquery.job.query.QueryJobConfig] = None, job_id: Optional[str] = None, job_id_prefix: Optional[str] = None, location: Optional[str] = None, project: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None, job_retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE350460>, api_method: Union[str, google.cloud.bigquery.enums.QueryApiMethod] = <QueryApiMethod.INSERT: 'INSERT'>) -> google.cloud.bigquery.job.query.QueryJob\n",
      " |      Run a SQL query.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationquery\n",
      " |      \n",
      " |      Args:\n",
      " |          query (str):\n",
      " |              SQL query to be executed. Defaults to the standard SQL\n",
      " |              dialect. Use the ``job_config`` parameter to change dialects.\n",
      " |          job_config (Optional[google.cloud.bigquery.job.QueryJobConfig]):\n",
      " |              Extra configuration options for the job.\n",
      " |              To override any options that were previously set in\n",
      " |              the ``default_query_job_config`` given to the\n",
      " |              ``Client`` constructor, manually set those options to ``None``,\n",
      " |              or whatever value is preferred.\n",
      " |          job_id (Optional[str]): ID to use for the query job.\n",
      " |          job_id_prefix (Optional[str]):\n",
      " |              The prefix to use for a randomly generated job ID. This parameter\n",
      " |              will be ignored if a ``job_id`` is also given.\n",
      " |          location (Optional[str]):\n",
      " |              Location where to run the job. Must match the location of the\n",
      " |              table used in the query as well as the destination table.\n",
      " |          project (Optional[str]):\n",
      " |              Project ID of the project of where to run the job. Defaults\n",
      " |              to the client's project.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.  This only applies to making RPC\n",
      " |              calls.  It isn't used to retry failed jobs.  This has\n",
      " |              a reasonable default that should only be overridden\n",
      " |              with care.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          job_retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry failed jobs.  The default retries\n",
      " |              rate-limit-exceeded errors.  Passing ``None`` disables\n",
      " |              job retry.\n",
      " |      \n",
      " |              Not all jobs can be retried.  If ``job_id`` is\n",
      " |              provided, then the job returned by the query will not\n",
      " |              be retryable, and an exception will be raised if a\n",
      " |              non-``None`` (and non-default) value for ``job_retry``\n",
      " |              is also provided.\n",
      " |      \n",
      " |              Note that errors aren't detected until ``result()`` is\n",
      " |              called on the job returned. The ``job_retry``\n",
      " |              specified here becomes the default ``job_retry`` for\n",
      " |              ``result()``, where it can also be specified.\n",
      " |          api_method (Union[str, enums.QueryApiMethod]):\n",
      " |              Method with which to start the query job.\n",
      " |      \n",
      " |              See :class:`google.cloud.bigquery.enums.QueryApiMethod` for\n",
      " |              details on the difference between the query start methods.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.job.QueryJob: A new query job instance.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``job_config`` is not an instance of\n",
      " |              :class:`~google.cloud.bigquery.job.QueryJobConfig`\n",
      " |              class, or if both ``job_id`` and non-``None`` non-default\n",
      " |              ``job_retry`` are provided.\n",
      " |  \n",
      " |  query_and_wait(self, query, *, job_config: Optional[google.cloud.bigquery.job.query.QueryJobConfig] = None, location: Optional[str] = None, project: Optional[str] = None, api_timeout: Optional[float] = None, wait_timeout: Optional[float] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, job_retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE350460>, page_size: Optional[int] = None, max_results: Optional[int] = None) -> google.cloud.bigquery.table.RowIterator\n",
      " |      Run the query, wait for it to finish, and return the results.\n",
      " |      \n",
      " |      While ``jobCreationMode=JOB_CREATION_OPTIONAL`` is in preview in the\n",
      " |      ``jobs.query`` REST API, use the default ``jobCreationMode`` unless\n",
      " |      the environment variable ``QUERY_PREVIEW_ENABLED=true``. After\n",
      " |      ``jobCreationMode`` is GA, this method will always use\n",
      " |      ``jobCreationMode=JOB_CREATION_OPTIONAL``. See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/query\n",
      " |      \n",
      " |      Args:\n",
      " |          query (str):\n",
      " |              SQL query to be executed. Defaults to the standard SQL\n",
      " |              dialect. Use the ``job_config`` parameter to change dialects.\n",
      " |          job_config (Optional[google.cloud.bigquery.job.QueryJobConfig]):\n",
      " |              Extra configuration options for the job.\n",
      " |              To override any options that were previously set in\n",
      " |              the ``default_query_job_config`` given to the\n",
      " |              ``Client`` constructor, manually set those options to ``None``,\n",
      " |              or whatever value is preferred.\n",
      " |          location (Optional[str]):\n",
      " |              Location where to run the job. Must match the location of the\n",
      " |              table used in the query as well as the destination table.\n",
      " |          project (Optional[str]):\n",
      " |              Project ID of the project of where to run the job. Defaults\n",
      " |              to the client's project.\n",
      " |          api_timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |          wait_timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the query to finish. If the\n",
      " |              query doesn't finish before this timeout, the client attempts\n",
      " |              to cancel the query.\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.  This only applies to making RPC\n",
      " |              calls.  It isn't used to retry failed jobs.  This has\n",
      " |              a reasonable default that should only be overridden\n",
      " |              with care.\n",
      " |          job_retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry failed jobs.  The default retries\n",
      " |              rate-limit-exceeded errors.  Passing ``None`` disables\n",
      " |              job retry. Not all jobs can be retried.\n",
      " |          page_size (Optional[int]):\n",
      " |              The maximum number of rows in each page of results from this\n",
      " |              request. Non-positive values are ignored.\n",
      " |          max_results (Optional[int]):\n",
      " |              The maximum total number of rows from this request.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.table.RowIterator:\n",
      " |              Iterator of row data\n",
      " |              :class:`~google.cloud.bigquery.table.Row`-s. During each\n",
      " |              page, the iterator will have the ``total_rows`` attribute\n",
      " |              set, which counts the total number of rows **in the result\n",
      " |              set** (this is distinct from the total number of rows in the\n",
      " |              current page: ``iterator.page.num_items``).\n",
      " |      \n",
      " |              If the query is a special query that produces no results, e.g.\n",
      " |              a DDL query, an ``_EmptyRowIterator`` instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |          TypeError:\n",
      " |              If ``job_config`` is not an instance of\n",
      " |              :class:`~google.cloud.bigquery.job.QueryJobConfig`\n",
      " |              class.\n",
      " |  \n",
      " |  schema_from_json(self, file_or_path: 'PathType') -> List[google.cloud.bigquery.schema.SchemaField]\n",
      " |      Takes a file object or file path that contains json that describes\n",
      " |      a table schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[SchemaField]:\n",
      " |              List of :class:`~google.cloud.bigquery.schema.SchemaField` objects.\n",
      " |  \n",
      " |  schema_to_json(self, schema_list: Sequence[google.cloud.bigquery.schema.SchemaField], destination: 'PathType')\n",
      " |      Takes a list of schema field objects.\n",
      " |      \n",
      " |      Serializes the list of schema field objects as json to a file.\n",
      " |      \n",
      " |      Destination is a file path or a file object.\n",
      " |  \n",
      " |  set_iam_policy(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], policy: google.api_core.iam.Policy, updateMask: Optional[str] = None, retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.api_core.iam.Policy\n",
      " |  \n",
      " |  test_iam_permissions(self, table: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], permissions: Sequence[str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> Dict[str, Any]\n",
      " |  \n",
      " |  update_dataset(self, dataset: google.cloud.bigquery.dataset.Dataset, fields: Sequence[str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.dataset.Dataset\n",
      " |      Change some fields of a dataset.\n",
      " |      \n",
      " |      Use ``fields`` to specify which fields to update. At least one field\n",
      " |      must be provided. If a field is listed in ``fields`` and is ``None`` in\n",
      " |      ``dataset``, it will be deleted.\n",
      " |      \n",
      " |      If ``dataset.etag`` is not ``None``, the update will only\n",
      " |      succeed if the dataset on the server has the same ETag. Thus\n",
      " |      reading a dataset with ``get_dataset``, changing its fields,\n",
      " |      and then passing it to ``update_dataset`` will ensure that the changes\n",
      " |      will only be saved if no modifications to the dataset occurred\n",
      " |      since the read.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset (google.cloud.bigquery.dataset.Dataset):\n",
      " |              The dataset to update.\n",
      " |          fields (Sequence[str]):\n",
      " |              The properties of ``dataset`` to change. These are strings\n",
      " |              corresponding to the properties of\n",
      " |              :class:`~google.cloud.bigquery.dataset.Dataset`.\n",
      " |      \n",
      " |              For example, to update the default expiration times, specify\n",
      " |              both properties in the ``fields`` argument:\n",
      " |      \n",
      " |              .. code-block:: python\n",
      " |      \n",
      " |                  bigquery_client.update_dataset(\n",
      " |                      dataset,\n",
      " |                      [\n",
      " |                          \"default_partition_expiration_ms\",\n",
      " |                          \"default_table_expiration_ms\",\n",
      " |                      ]\n",
      " |                  )\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              How to retry the RPC.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.dataset.Dataset:\n",
      " |              The modified ``Dataset`` instance.\n",
      " |  \n",
      " |  update_model(self, model: google.cloud.bigquery.model.Model, fields: Sequence[str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.model.Model\n",
      " |      [Beta] Change some fields of a model.\n",
      " |      \n",
      " |      Use ``fields`` to specify which fields to update. At least one field\n",
      " |      must be provided. If a field is listed in ``fields`` and is ``None``\n",
      " |      in ``model``, the field value will be deleted.\n",
      " |      \n",
      " |      If ``model.etag`` is not ``None``, the update will only succeed if\n",
      " |      the model on the server has the same ETag. Thus reading a model with\n",
      " |      ``get_model``, changing its fields, and then passing it to\n",
      " |      ``update_model`` will ensure that the changes will only be saved if\n",
      " |      no modifications to the model occurred since the read.\n",
      " |      \n",
      " |      Args:\n",
      " |          model (google.cloud.bigquery.model.Model): The model to update.\n",
      " |          fields (Sequence[str]):\n",
      " |              The properties of ``model`` to change. These are strings\n",
      " |              corresponding to the properties of\n",
      " |              :class:`~google.cloud.bigquery.model.Model`.\n",
      " |      \n",
      " |              For example, to update the descriptive properties of the model,\n",
      " |              specify them in the ``fields`` argument:\n",
      " |      \n",
      " |              .. code-block:: python\n",
      " |      \n",
      " |                  bigquery_client.update_model(\n",
      " |                      model, [\"description\", \"friendly_name\"]\n",
      " |                  )\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              A description of how to retry the API call.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.model.Model:\n",
      " |              The model resource returned from the API call.\n",
      " |  \n",
      " |  update_routine(self, routine: google.cloud.bigquery.routine.routine.Routine, fields: Sequence[str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.routine.routine.Routine\n",
      " |      [Beta] Change some fields of a routine.\n",
      " |      \n",
      " |      Use ``fields`` to specify which fields to update. At least one field\n",
      " |      must be provided. If a field is listed in ``fields`` and is ``None``\n",
      " |      in ``routine``, the field value will be deleted.\n",
      " |      \n",
      " |      .. warning::\n",
      " |         During beta, partial updates are not supported. You must provide\n",
      " |         all fields in the resource.\n",
      " |      \n",
      " |      If :attr:`~google.cloud.bigquery.routine.Routine.etag` is not\n",
      " |      ``None``, the update will only succeed if the resource on the server\n",
      " |      has the same ETag. Thus reading a routine with\n",
      " |      :func:`~google.cloud.bigquery.client.Client.get_routine`, changing\n",
      " |      its fields, and then passing it to this method will ensure that the\n",
      " |      changes will only be saved if no modifications to the resource\n",
      " |      occurred since the read.\n",
      " |      \n",
      " |      Args:\n",
      " |          routine (google.cloud.bigquery.routine.Routine):\n",
      " |              The routine to update.\n",
      " |          fields (Sequence[str]):\n",
      " |              The fields of ``routine`` to change, spelled as the\n",
      " |              :class:`~google.cloud.bigquery.routine.Routine` properties.\n",
      " |      \n",
      " |              For example, to update the description property of the routine,\n",
      " |              specify it in the ``fields`` argument:\n",
      " |      \n",
      " |              .. code-block:: python\n",
      " |      \n",
      " |                  bigquery_client.update_routine(\n",
      " |                      routine, [\"description\"]\n",
      " |                  )\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              A description of how to retry the API call.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.routine.Routine:\n",
      " |              The routine resource returned from the API call.\n",
      " |  \n",
      " |  update_table(self, table: google.cloud.bigquery.table.Table, fields: Sequence[str], retry: google.api_core.retry.retry_unary.Retry = <google.api_core.retry.retry_unary.Retry object at 0x00000266FE3504C0>, timeout: Optional[float] = None) -> google.cloud.bigquery.table.Table\n",
      " |      Change some fields of a table.\n",
      " |      \n",
      " |      Use ``fields`` to specify which fields to update. At least one field\n",
      " |      must be provided. If a field is listed in ``fields`` and is ``None``\n",
      " |      in ``table``, the field value will be deleted.\n",
      " |      \n",
      " |      If ``table.etag`` is not ``None``, the update will only succeed if\n",
      " |      the table on the server has the same ETag. Thus reading a table with\n",
      " |      ``get_table``, changing its fields, and then passing it to\n",
      " |      ``update_table`` will ensure that the changes will only be saved if\n",
      " |      no modifications to the table occurred since the read.\n",
      " |      \n",
      " |      Args:\n",
      " |          table (google.cloud.bigquery.table.Table): The table to update.\n",
      " |          fields (Sequence[str]):\n",
      " |              The fields of ``table`` to change, spelled as the\n",
      " |              :class:`~google.cloud.bigquery.table.Table` properties.\n",
      " |      \n",
      " |              For example, to update the descriptive properties of the table,\n",
      " |              specify them in the ``fields`` argument:\n",
      " |      \n",
      " |              .. code-block:: python\n",
      " |      \n",
      " |                  bigquery_client.update_table(\n",
      " |                      table,\n",
      " |                      [\"description\", \"friendly_name\"]\n",
      " |                  )\n",
      " |          retry (Optional[google.api_core.retry.Retry]):\n",
      " |              A description of how to retry the API call.\n",
      " |          timeout (Optional[float]):\n",
      " |              The number of seconds to wait for the underlying HTTP transport\n",
      " |              before using ``retry``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.table.Table:\n",
      " |              The table resource returned from the API call.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  location\n",
      " |      Default location for jobs / datasets / tables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  default_load_job_config\n",
      " |      Default ``LoadJobConfig``.\n",
      " |      Will be merged into job configs passed into the ``load_table_*`` methods.\n",
      " |  \n",
      " |  default_query_job_config\n",
      " |      Default ``QueryJobConfig`` or ``None``.\n",
      " |      \n",
      " |      Will be merged into job configs passed into the ``query`` or\n",
      " |      ``query_and_wait`` methods.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  SCOPE = ('https://www.googleapis.com/auth/cloud-platform',)\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from google.cloud.client.Client:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Explicitly state that clients are not pickleable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from google.cloud.client._ClientFactoryMixin:\n",
      " |  \n",
      " |  from_service_account_info(info, *args, **kwargs) from builtins.type\n",
      " |      Factory to retrieve JSON credentials while creating client.\n",
      " |      \n",
      " |      :type info: dict\n",
      " |      :param info:\n",
      " |          The JSON object with a private key and other credentials\n",
      " |          information (downloaded from the Google APIs console).\n",
      " |      \n",
      " |      :type args: tuple\n",
      " |      :param args: Remaining positional arguments to pass to constructor.\n",
      " |      \n",
      " |      :param kwargs: Remaining keyword arguments to pass to constructor.\n",
      " |      \n",
      " |      :rtype: :class:`_ClientFactoryMixin`\n",
      " |      :returns: The client created with the retrieved JSON credentials.\n",
      " |      :raises TypeError: if there is a conflict with the kwargs\n",
      " |               and the credentials created by the factory.\n",
      " |  \n",
      " |  from_service_account_json(json_credentials_path, *args, **kwargs) from builtins.type\n",
      " |      Factory to retrieve JSON credentials while creating client.\n",
      " |      \n",
      " |      :type json_credentials_path: str\n",
      " |      :param json_credentials_path: The path to a private key file (this file\n",
      " |                                    was given to you when you created the\n",
      " |                                    service account). This file must contain\n",
      " |                                    a JSON object with a private key and\n",
      " |                                    other credentials information (downloaded\n",
      " |                                    from the Google APIs console).\n",
      " |      \n",
      " |      :type args: tuple\n",
      " |      :param args: Remaining positional arguments to pass to constructor.\n",
      " |      \n",
      " |      :param kwargs: Remaining keyword arguments to pass to constructor.\n",
      " |      \n",
      " |      :rtype: :class:`_ClientFactoryMixin`\n",
      " |      :returns: The client created with the retrieved JSON credentials.\n",
      " |      :raises TypeError: if there is a conflict with the kwargs\n",
      " |               and the credentials created by the factory.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from google.cloud.client._ClientFactoryMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (bgclient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a367d56-b17a-4f24-967b-539f45848956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Table in module google.cloud.bigquery.table:\n",
      "\n",
      "class Table(_TableBase)\n",
      " |  Table(table_ref, schema=None) -> None\n",
      " |  \n",
      " |  Tables represent a set of rows whose values correspond to a schema.\n",
      " |  \n",
      " |  See\n",
      " |  https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#resource-table\n",
      " |  \n",
      " |  Args:\n",
      " |      table_ref (Union[google.cloud.bigquery.table.TableReference, str]):\n",
      " |          A pointer to a table. If ``table_ref`` is a string, it must\n",
      " |          included a project ID, dataset ID, and table ID, each separated\n",
      " |          by ``.``.\n",
      " |      schema (Optional[Sequence[Union[                 :class:`~google.cloud.bigquery.schema.SchemaField`,                 Mapping[str, Any]         ]]]):\n",
      " |          The table's schema. If any item is a mapping, its content must be\n",
      " |          compatible with\n",
      " |          :meth:`~google.cloud.bigquery.schema.SchemaField.from_api_repr`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Table\n",
      " |      _TableBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, table_ref, schema=None) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  to_api_repr(self) -> dict\n",
      " |      Constructs the API resource of this table\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict[str, object]: Table represented as an API resource\n",
      " |  \n",
      " |  to_bqstorage(self) -> str\n",
      " |      Construct a BigQuery Storage API representation of this table.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: A reference to this table in the BigQuery Storage API.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_api_repr(resource: dict) -> 'Table' from builtins.type\n",
      " |      Factory: construct a table given its API representation\n",
      " |      \n",
      " |      Args:\n",
      " |          resource (Dict[str, object]):\n",
      " |              Table resource representation from the API\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.table.Table: Table parsed from ``resource``.\n",
      " |      \n",
      " |      Raises:\n",
      " |          KeyError:\n",
      " |              If the ``resource`` lacks the key ``'tableReference'``, or if\n",
      " |              the ``dict`` stored within the key ``'tableReference'`` lacks\n",
      " |              the keys ``'tableId'``, ``'projectId'``, or ``'datasetId'``.\n",
      " |  \n",
      " |  from_string(full_table_id: str) -> 'Table' from builtins.type\n",
      " |      Construct a table from fully-qualified table ID.\n",
      " |      \n",
      " |      Args:\n",
      " |          full_table_id (str):\n",
      " |              A fully-qualified table ID in standard SQL format. Must\n",
      " |              included a project ID, dataset ID, and table ID, each\n",
      " |              separated by ``.``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Table: Table parsed from ``full_table_id``.\n",
      " |      \n",
      " |      Examples:\n",
      " |          >>> Table.from_string('my-project.mydataset.mytable')\n",
      " |          Table(TableRef...(D...('my-project', 'mydataset'), 'mytable'))\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError:\n",
      " |              If ``full_table_id`` is not a fully-qualified table ID in\n",
      " |              standard SQL format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  clone_definition\n",
      " |      Information about the clone. This value is set via clone creation.\n",
      " |      \n",
      " |      See: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#Table.FIELDS.clone_definition\n",
      " |  \n",
      " |  created\n",
      " |      Union[datetime.datetime, None]: Datetime at which the table was\n",
      " |      created (:data:`None` until set from the server).\n",
      " |  \n",
      " |  etag\n",
      " |      Union[str, None]: ETag for the table resource (:data:`None` until\n",
      " |      set from the server).\n",
      " |  \n",
      " |  full_table_id\n",
      " |      Union[str, None]: ID for the table (:data:`None` until set from the\n",
      " |      server).\n",
      " |      \n",
      " |      In the format ``project-id:dataset_id.table_id``.\n",
      " |  \n",
      " |  location\n",
      " |      Union[str, None]: Location in which the table is hosted\n",
      " |      \n",
      " |      Defaults to :data:`None`.\n",
      " |  \n",
      " |  modified\n",
      " |      Union[datetime.datetime, None]: Datetime at which the table was last\n",
      " |      modified (:data:`None` until set from the server).\n",
      " |  \n",
      " |  mview_last_refresh_time\n",
      " |      Optional[datetime.datetime]: Datetime at which the materialized view was last\n",
      " |      refreshed (:data:`None` until set from the server).\n",
      " |  \n",
      " |  num_bytes\n",
      " |      Union[int, None]: The size of the table in bytes (:data:`None` until\n",
      " |      set from the server).\n",
      " |  \n",
      " |  num_rows\n",
      " |      Union[int, None]: The number of rows in the table (:data:`None`\n",
      " |      until set from the server).\n",
      " |  \n",
      " |  reference\n",
      " |      A :class:`~google.cloud.bigquery.table.TableReference` pointing to\n",
      " |      this table.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.table.TableReference: pointer to this table.\n",
      " |  \n",
      " |  self_link\n",
      " |      Union[str, None]: URL for the table resource (:data:`None` until set\n",
      " |      from the server).\n",
      " |  \n",
      " |  snapshot_definition\n",
      " |      Information about the snapshot. This value is set via snapshot creation.\n",
      " |      \n",
      " |      See: https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#Table.FIELDS.snapshot_definition\n",
      " |  \n",
      " |  streaming_buffer\n",
      " |      google.cloud.bigquery.StreamingBuffer: Information about a table's\n",
      " |      streaming buffer.\n",
      " |  \n",
      " |  table_constraints\n",
      " |      Tables Primary Key and Foreign Key information.\n",
      " |  \n",
      " |  table_type\n",
      " |      Union[str, None]: The type of the table (:data:`None` until set from\n",
      " |      the server).\n",
      " |      \n",
      " |      Possible values are ``'TABLE'``, ``'VIEW'``, ``'MATERIALIZED_VIEW'`` or\n",
      " |      ``'EXTERNAL'``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  clustering_fields\n",
      " |      Union[List[str], None]: Fields defining clustering for the table\n",
      " |      \n",
      " |      (Defaults to :data:`None`).\n",
      " |      \n",
      " |      Clustering fields are immutable after table creation.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         BigQuery supports clustering for both partitioned and\n",
      " |         non-partitioned tables.\n",
      " |  \n",
      " |  description\n",
      " |      Union[str, None]: Description of the table (defaults to\n",
      " |      :data:`None`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: For invalid value types.\n",
      " |  \n",
      " |  encryption_configuration\n",
      " |      google.cloud.bigquery.encryption_configuration.EncryptionConfiguration: Custom\n",
      " |      encryption configuration for the table.\n",
      " |      \n",
      " |      Custom encryption configuration (e.g., Cloud KMS keys) or :data:`None`\n",
      " |      if using default encryption.\n",
      " |      \n",
      " |      See `protecting data with Cloud KMS keys\n",
      " |      <https://cloud.google.com/bigquery/docs/customer-managed-encryption>`_\n",
      " |      in the BigQuery documentation.\n",
      " |  \n",
      " |  expires\n",
      " |      Union[datetime.datetime, None]: Datetime at which the table will be\n",
      " |      deleted.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: For invalid value types.\n",
      " |  \n",
      " |  external_data_configuration\n",
      " |      Union[google.cloud.bigquery.ExternalConfig, None]: Configuration for\n",
      " |      an external data source (defaults to :data:`None`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: For invalid value types.\n",
      " |  \n",
      " |  friendly_name\n",
      " |      Union[str, None]: Title of the table (defaults to :data:`None`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: For invalid value types.\n",
      " |  \n",
      " |  labels\n",
      " |      Dict[str, str]: Labels for the table.\n",
      " |      \n",
      " |      This method always returns a dict. To change a table's labels,\n",
      " |      modify the dict, then call ``Client.update_table``. To delete a\n",
      " |      label, set its value to :data:`None` before updating.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If ``value`` type is invalid.\n",
      " |  \n",
      " |  mview_enable_refresh\n",
      " |      Optional[bool]: Enable automatic refresh of the materialized view\n",
      " |      when the base table is updated. The default value is :data:`True`.\n",
      " |  \n",
      " |  mview_query\n",
      " |      Optional[str]: SQL query defining the table as a materialized\n",
      " |      view (defaults to :data:`None`).\n",
      " |  \n",
      " |  mview_refresh_interval\n",
      " |      Optional[datetime.timedelta]: The maximum frequency at which this\n",
      " |      materialized view will be refreshed. The default value is 1800000\n",
      " |      milliseconds (30 minutes).\n",
      " |  \n",
      " |  partition_expiration\n",
      " |      Union[int, None]: Expiration time in milliseconds for a partition.\n",
      " |      \n",
      " |      If :attr:`partition_expiration` is set and :attr:`type_` is\n",
      " |      not set, :attr:`type_` will default to\n",
      " |      :attr:`~google.cloud.bigquery.table.TimePartitioningType.DAY`.\n",
      " |  \n",
      " |  partitioning_type\n",
      " |      Union[str, None]: Time partitioning of the table if it is\n",
      " |      partitioned (Defaults to :data:`None`).\n",
      " |  \n",
      " |  range_partitioning\n",
      " |      Optional[google.cloud.bigquery.table.RangePartitioning]:\n",
      " |      Configures range-based partitioning for a table.\n",
      " |      \n",
      " |      .. note::\n",
      " |          **Beta**. The integer range partitioning feature is in a\n",
      " |          pre-release state and might change or have limited support.\n",
      " |      \n",
      " |      Only specify at most one of\n",
      " |      :attr:`~google.cloud.bigquery.table.Table.time_partitioning` or\n",
      " |      :attr:`~google.cloud.bigquery.table.Table.range_partitioning`.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError:\n",
      " |              If the value is not\n",
      " |              :class:`~google.cloud.bigquery.table.RangePartitioning` or\n",
      " |              :data:`None`.\n",
      " |  \n",
      " |  require_partition_filter\n",
      " |      bool: If set to true, queries over the partitioned table require a\n",
      " |      partition filter that can be used for partition elimination to be\n",
      " |      specified.\n",
      " |  \n",
      " |  schema\n",
      " |      Sequence[Union[                 :class:`~google.cloud.bigquery.schema.SchemaField`,                 Mapping[str, Any]         ]]:\n",
      " |          Table's schema.\n",
      " |      \n",
      " |      Raises:\n",
      " |          Exception:\n",
      " |              If ``schema`` is not a sequence, or if any item in the sequence\n",
      " |              is not a :class:`~google.cloud.bigquery.schema.SchemaField`\n",
      " |              instance or a compatible mapping representation of the field.\n",
      " |  \n",
      " |  time_partitioning\n",
      " |      Optional[google.cloud.bigquery.table.TimePartitioning]: Configures time-based\n",
      " |      partitioning for a table.\n",
      " |      \n",
      " |      Only specify at most one of\n",
      " |      :attr:`~google.cloud.bigquery.table.Table.time_partitioning` or\n",
      " |      :attr:`~google.cloud.bigquery.table.Table.range_partitioning`.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError:\n",
      " |              If the value is not\n",
      " |              :class:`~google.cloud.bigquery.table.TimePartitioning` or\n",
      " |              :data:`None`.\n",
      " |  \n",
      " |  view_query\n",
      " |      Union[str, None]: SQL query defining the table as a view (defaults\n",
      " |      to :data:`None`).\n",
      " |      \n",
      " |      By default, the query is treated as Standard SQL. To use Legacy\n",
      " |      SQL, set :attr:`view_use_legacy_sql` to :data:`True`.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: For invalid value types.\n",
      " |  \n",
      " |  view_use_legacy_sql\n",
      " |      bool: Specifies whether to execute the view with Legacy or Standard SQL.\n",
      " |      \n",
      " |      This boolean specifies whether to execute the view with Legacy SQL\n",
      " |      (:data:`True`) or Standard SQL (:data:`False`). The client side default is\n",
      " |      :data:`False`. The server-side default is :data:`True`. If this table is\n",
      " |      not a view, :data:`None` is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: For invalid value types.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _TableBase:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _TableBase:\n",
      " |  \n",
      " |  dataset_id\n",
      " |      ID of dataset containing the table.\n",
      " |  \n",
      " |  path\n",
      " |      URL path for the table's APIs.\n",
      " |  \n",
      " |  project\n",
      " |      Project bound to the table.\n",
      " |  \n",
      " |  table_id\n",
      " |      The table ID.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _TableBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help (bigquery.Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fde850c8-81da-4256-8dc9-128820a6d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<google.cloud.bigquery.table._EmptyRowIterator object at 0x00000266A18CCFD0>\n"
     ]
    }
   ],
   "source": [
    "table0 = f\"{project}.{dataset}.udemy_retail_gcptableorders\"\n",
    "delete0table = bigquery.Table.from_string (table0)\n",
    "delete0query = bgclient.delete_table (delete0table, not_found_ok=True)\n",
    "\n",
    "delete1query = bgclient.query (f\"DROP TABLE IF EXISTS {table1}\")\n",
    "\n",
    "delete2query = bgclient.delete_table (table2, not_found_ok=True)\n",
    "\n",
    "print (delete0query)\n",
    "#delete0result = delete0query.result ()\n",
    "delete1result = delete1query.result ()\n",
    "print (delete1result)\n",
    "#delete2result = delete2query.result ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4ba22-ca7c-414a-8f01-37e0ab851a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
